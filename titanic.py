import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.cluster import KMeans
from sklearn.metrics import accuracy_score



# üì•  Carregar o dataset Titanic
df = pd.read_csv("datasets/titanic/train.csv")

# Selecionar apenas colunas relevantes e tratar valores faltantes
df = df[['Survived', 'Pclass', 'Sex', 'Age', 'Fare']].dropna()

# Converter 'Sex' para valores num√©ricos (0 = Male, 1 = Female)
df['Sex'] = LabelEncoder().fit_transform(df['Sex'])

# Normalizar apenas as colunas num√©ricas (Age, Fare, Pclass)
scaler = MinMaxScaler()
df[['Age', 'Fare', 'Pclass']] = scaler.fit_transform(df[['Age', 'Fare', 'Pclass']])

# Exibir os primeiros dados
print(df.head())

# Preparar os dados para treinamento e teste
kf = KFold(n_splits=10, shuffle=True, random_state=42)
X = df.iloc[:, 1:].values  # Atributos
y = df.iloc[:, 0].values   # Labels

# Modelos de Machine Learning
tree_gini = DecisionTreeClassifier(criterion="gini")
tree_entropy = DecisionTreeClassifier(criterion="entropy")
knn_5 = KNeighborsClassifier(n_neighbors=5)
knn_10 = KNeighborsClassifier(n_neighbors=10)
mlp_relu = MLPClassifier(hidden_layer_sizes=(100,), activation="relu", max_iter=2000, random_state=42)
mlp_tanh = MLPClassifier(hidden_layer_sizes=(100,), activation="tanh", max_iter=2000, random_state=42)
kmeans = KMeans(n_clusters=len(np.unique(y)), random_state=42)

# Dicion√°rio com os modelos
models = {
    "Tree (Gini)": tree_gini,
    "Tree (Entropy)": tree_entropy,
    "kNN (k=5)": knn_5,
    "kNN (k=10)": knn_10,
    "MLP (ReLU)": mlp_relu,
    "MLP (Tanh)": mlp_tanh,
    "K-Means": kmeans
}

# Rodar os Modelos e Calcular as M√©tricas
results = {}

# Vari√°veis para salvar a curva de erro das MLPs
loss_curve_relu = []
loss_curve_tanh = []

for name, model in models.items():
    accuracies = []
    
    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        # Treinar e testar o modelo
        if "K-Means" in name:
            model.fit(X_train)
            y_pred = model.predict(X_test)
            y_pred = np.array([y_train[i] for i in y_pred])  # Ajustar r√≥tulos
        else:
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
        
        # Calcular acur√°cia
        acc = accuracy_score(y_test, y_pred)
        accuracies.append(acc)

        # Salvar a curva de erro para as MLPs
        if name == "MLP (ReLU)":
            loss_curve_relu.append(model.loss_curve_)
        elif name == "MLP (Tanh)":
            loss_curve_tanh.append(model.loss_curve_)
    
    # M√©dia dos 10 folds
    results[name] = np.mean(accuracies) * 100

# üìä  Exibir os resultados
df_results = pd.DataFrame.from_dict(results, orient="index", columns=["Accuracy (%)"])
print(df_results)

# Plotar as curvas de erro para MLP (ReLU) e MLP (Tanh)
# As curvas de erro podem ter comprimentos diferentes, ent√£o plotaremos todas as itera√ß√µes separadamente

for curve in loss_curve_relu:
    plt.plot(curve, label="MLP (ReLU) - Folds")
for curve in loss_curve_tanh:
    plt.plot(curve, label="MLP (Tanh) - Folds")

plt.xlabel("√âpocas")
plt.ylabel("Erro")
plt.title("Evolu√ß√£o do Erro no Treinamento das MLPs")
plt.legend()
plt.show()


# Plotando o gr√°fico de barras com seaborn
plt.figure(figsize=(10, 5))  # Tamanho da figura
sns.barplot(x=df_results.index, y=df_results["Accuracy (%)"])  # Plotando a acur√°cia m√©dia por modelo
plt.xticks(rotation=45)  # Rotacionando os r√≥tulos no eixo X
plt.title("Compara√ß√£o de Algoritmos (Acur√°cia M√©dia)")  # T√≠tulo do gr√°fico
plt.ylabel("Acur√°cia (%)")  # R√≥tulo do eixo Y
plt.show()  # Exibir o gr√°fico
